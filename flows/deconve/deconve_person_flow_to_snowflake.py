import os
import csv
import time
import tempfile
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List
from dotenv import load_dotenv
from prefect import flow, task
from prefect.logging import get_run_logger
from prefect.cache_policies import NONE as NO_CACHE
from prefect.artifacts import create_table_artifact
from prefect.blocks.system import Secret
from prefect.client.schemas.schedules import CronSchedule

# Imports dos mÃ³dulos de conexÃ£o
import sys

sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

from shared.connections.deconve import (  # noqa: E402
    authenticate_deconve, get_units, get_unit_details, get_people_counter_report
)
from shared.connections.snowflake import (  # noqa: E402
    connect_snowflake, create_table_if_not_exists,
    merge_csv_to_snowflake, close_snowflake_connection,
    DECONVE_TABLES_SCHEMAS
)
from shared.alerts import (  # noqa: E402
    send_flow_success_alert, send_flow_error_alert
)

# Carrega variÃ¡veis de ambiente
load_dotenv()


def calculate_date_range(days_back: int = 3):
    """
    Calcula range de datas para processamento retroativo

    Args:
        days_back: NÃºmero de dias para voltar (padrÃ£o: 3)

    Returns:
        Tuple (start_date, end_date) no formato ISO com timezone
    """
    # Data/hora de D-1 (ontem)
    end_datetime = datetime.now() - timedelta(days=1)

    # Data/hora inicial (X dias atrÃ¡s a partir de D-1)
    start_datetime = end_datetime - timedelta(days=days_back)

    # Formata para ISO com timezone -03:00 (horÃ¡rio de BrasÃ­lia)
    # InÃ­cio do dia
    start_date = start_datetime.replace(hour=0, minute=0, second=0, microsecond=0).strftime("%Y-%m-%dT%H:%M:%S-03:00")

    # Final do dia D-1 (ontem)
    end_date = end_datetime.replace(hour=23, minute=59, second=59, microsecond=0).strftime("%Y-%m-%dT%H:%M:%S-03:00")

    return start_date, end_date


@task(cache_policy=NO_CACHE)
def get_all_cameras(access_token: str, units_data: Dict[str, Any]) -> List[str]:
    """
    ObtÃ©m lista de todos os IDs de cÃ¢meras de todas as unidades

    Args:
        access_token: Token de acesso da API
        units_data: Dados das unidades

    Returns:
        Lista de IDs de cÃ¢meras
    """
    logger = get_run_logger()

    camera_ids = []
    items = units_data.get('items', [])

    logger.info(f"ðŸŽ¥ Buscando cÃ¢meras de {len(items)} unidade(s)...")

    for idx, item in enumerate(items, 1):
        unit_id = item.get('id', '')
        unit_name = item.get('name', '')

        try:
            logger.info(f"  [{idx}/{len(items)}] Buscando cÃ¢meras de {unit_name}...")
            unit_details = get_unit_details.fn(access_token, unit_id)
            videos = unit_details.get('videos', [])

            for video in videos:
                camera_id = video.get('id')
                if camera_id:
                    camera_ids.append(camera_id)

            logger.info(f"    âœ… {len(videos)} cÃ¢mera(s) encontrada(s)")

        except Exception as e:
            logger.error(f"    âŒ Erro ao buscar cÃ¢meras da unit {unit_id}: {str(e)}")
            continue

    logger.info(f"âœ… Total de cÃ¢meras encontradas: {len(camera_ids)}")

    return camera_ids


@task(cache_policy=NO_CACHE)
def fetch_people_counter_data_with_pagination(
        access_token: str,
        video_id: str,
        start_date: str,
        end_date: str,
        group_by: str = "hour",
        limit: int = 1000
) -> List[Dict[str, Any]]:
    """
    Busca dados de contador de pessoas com paginaÃ§Ã£o automÃ¡tica

    Args:
        access_token: Token de acesso
        video_id: ID do vÃ­deo/cÃ¢mera
        start_date: Data inicial
        end_date: Data final
        group_by: Agrupamento
        limit: Limite por pÃ¡gina

    Returns:
        Lista completa de registros
    """
    logger = get_run_logger()

    all_items = []
    skip = 0
    page = 1

    while True:
        try:
            logger.info(f"    ðŸ“„ PÃ¡gina {page} (skip={skip})...")

            report_data = get_people_counter_report(
                access_token=access_token,
                video_id=video_id,
                start_date=start_date,
                end_date=end_date,
                group_by=group_by,
                limit=limit,
                skip=skip
            )

            items = report_data.get('items', [])
            has_more = report_data.get('has_more', False)
            total = report_data.get('total', 0)

            all_items.extend(items)

            logger.info(f"    âœ… {len(items)} registros na pÃ¡gina {page} (total acumulado: {len(all_items)}/{total})")

            if not has_more or len(items) == 0:
                break

            skip += limit
            page += 1

        except Exception as e:
            logger.error(f"    âŒ Erro na pÃ¡gina {page}: {str(e)}")
            break

    return all_items


@task(cache_policy=NO_CACHE)
def process_cameras_and_save_csv(
        access_token: str,
        camera_ids: List[str],
        start_date: str,
        end_date: str,
        output_path: str,
        group_by: str = "hour"
) -> Dict[str, Any]:
    """
    Processa todas as cÃ¢meras e salva dados de person flow em CSV

    Args:
        access_token: Token de acesso
        camera_ids: Lista de IDs das cÃ¢meras
        start_date: Data inicial
        end_date: Data final
        output_path: Caminho do arquivo CSV
        group_by: Agrupamento (hour, day, etc)

    Returns:
        Dict com estatÃ­sticas do processamento
    """
    logger = get_run_logger()

    try:
        # Cria diretÃ³rio de saÃ­da se nÃ£o existir
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        # Define as colunas do CSV (em UPPER CASE)
        fieldnames = ['ID_CAMERA', 'DT_FLUXO', 'NR_ENTRADA', 'NR_SAIDA', 'DT_CRIACAO']

        # Data/hora de criaÃ§Ã£o do registro (horÃ¡rio de BrasÃ­lia: UTC-3)
        dt_criacao = (datetime.now() - timedelta(hours=3)).strftime("%Y-%m-%d %H:%M:%S")

        total_cameras = len(camera_ids)
        total_records = 0

        logger.info(f"ðŸ’¾ Processando {total_cameras} cÃ¢mera(s)...")
        logger.info(f"ðŸ“… PerÃ­odo: {start_date} a {end_date}")

        # Escreve arquivo CSV
        with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

            # Escreve cabeÃ§alho
            writer.writeheader()

            # Processa cada cÃ¢mera
            for idx, camera_id in enumerate(camera_ids, 1):
                logger.info(f"\nðŸ“¹ [{idx}/{total_cameras}] CÃ¢mera: {camera_id[:8]}...")

                try:
                    # Busca dados com paginaÃ§Ã£o
                    items = fetch_people_counter_data_with_pagination(
                        access_token=access_token,
                        video_id=camera_id,
                        start_date=start_date,
                        end_date=end_date,
                        group_by=group_by
                    )

                    # Escreve registros no CSV
                    for item in items:
                        created_at = item.get('created_at', '')
                        direction_in = item.get('direction_in', {})
                        direction_out = item.get('direction_out', {})

                        nr_entrada = direction_in.get('total', 0)
                        nr_saida = direction_out.get('total', 0)

                        writer.writerow({
                            'ID_CAMERA': camera_id,
                            'DT_FLUXO': created_at,
                            'NR_ENTRADA': nr_entrada,
                            'NR_SAIDA': nr_saida,
                            'DT_CRIACAO': dt_criacao
                        })

                        total_records += 1

                    logger.info(f"  âœ… {len(items)} registro(s) salvos")

                except Exception as e:
                    logger.error(f"  âŒ Erro ao processar cÃ¢mera {camera_id}: {str(e)}")
                    # Continua processando as outras cÃ¢meras

        file_size = os.path.getsize(output_path)
        file_size_kb = file_size / 1024

        logger.info(f"\nâœ… Arquivo CSV criado: {output_path}")
        logger.info(f"ðŸ“Š {total_records} registro(s) de {total_cameras} cÃ¢mera(s) salvos ({file_size_kb:.2f} KB)")

        return {
            "status": "success",
            "rows_written": total_records,
            "cameras_processed": total_cameras,
            "file_path": output_path,
            "file_size_bytes": file_size
        }

    except Exception as e:
        logger.error(f"âŒ Erro ao processar cÃ¢meras: {str(e)}")
        raise


@flow(log_prints=True, name="deconve-person-flow-to-snowflake")
def deconve_person_flow_to_snowflake(
        # Deconve params
        api_key: Optional[str] = None,
        days_back: int = 3,  # Processa Ãºltimos 3 dias por padrÃ£o (retroativo)
        start_date: Optional[str] = None,  # Permite override manual
        end_date: Optional[str] = None,  # Permite override manual
        group_by: str = "hour",
        # Snowflake params
        snowflake_account: Optional[str] = None,
        snowflake_user: Optional[str] = None,
        snowflake_private_key: Optional[str] = None,
        snowflake_private_key_passphrase: Optional[str] = None,
        snowflake_warehouse: Optional[str] = None,
        snowflake_database: Optional[str] = None,
        snowflake_schema: Optional[str] = None,
        snowflake_role: Optional[str] = None,
        # Alertas
        send_alerts: bool = True,
        alert_group_id: Optional[str] = None
):
    """
    Flow: Deconve API -> Snowflake (DECONVE_PERSON_FLOW) com MERGE

    EstratÃ©gia anti-duplicaÃ§Ã£o:
    - Usa MERGE (UPSERT) no Snowflake
    - Chave composta: (ID_CAMERA, DT_FLOW)
    - Se registro jÃ¡ existe: ATUALIZA
    - Se registro nÃ£o existe: INSERE
    - Permite reprocessamento seguro de perÃ­odos retroativos

    Processo:
    1. Calcula range de datas (data atual - X dias)
    2. Autentica na API Deconve
    3. ObtÃ©m lista de todas as cÃ¢meras
    4. Para cada cÃ¢mera, busca relatÃ³rio de person flow (com paginaÃ§Ã£o)
    5. Salva em CSV temporÃ¡rio
    6. Cria tabela no Snowflake se nÃ£o existir (schema GOLD)
    7. Faz MERGE (UPSERT) do CSV no Snowflake
    8. Remove CSV temporÃ¡rio
    9. Envia alertas
    """
    logger = get_run_logger()
    start_time = time.time()

    logger.info("=" * 80)
    logger.info("ðŸš€ Iniciando flow Deconve API -> Snowflake (Person Flow)")
    logger.info("=" * 80)

    # 1. Carrega configuraÃ§Ãµes
    # Carrega API Key do Deconve via Prefect Block (Secret)
    if not api_key:
        try:
            logger.info("ðŸ” Carregando DECONVE_API_KEY do Prefect Block...")
            api_key_block = Secret.load("deconve-api-key")
            api_key = api_key_block.get()
            logger.info("âœ… API Key carregada do Block 'deconve-api-key'")
        except Exception as e:
            logger.warning(f"âš ï¸ Erro ao carregar Block 'deconve-api-key': {e}")
            logger.info("Tentando carregar do .env como fallback...")
            api_key = os.getenv("DECONVE_API_KEY")

    # Snowflake (mantÃ©m .env para estas variÃ¡veis por enquanto)
    snowflake_account = snowflake_account or os.getenv("SNOWFLAKE_ACCOUNT")
    snowflake_user = snowflake_user or os.getenv("SNOWFLAKE_USER")
    snowflake_private_key = snowflake_private_key or os.getenv("SNOWFLAKE_PRIVATE_KEY")
    snowflake_private_key_passphrase = snowflake_private_key_passphrase or os.getenv("SNOWFLAKE_PRIVATE_KEY_PASSPHRASE")
    snowflake_warehouse = snowflake_warehouse or os.getenv("SNOWFLAKE_WAREHOUSE")
    snowflake_database = snowflake_database or os.getenv("SNOWFLAKE_DATABASE")
    snowflake_role = snowflake_role or os.getenv("SNOWFLAKE_ROLE")

    # Carrega Schema especÃ­fico do Deconve via Prefect Block (Secret)
    if not snowflake_schema:
        try:
            logger.info("ðŸ” Carregando DECONVE_SNOWFLAKE_SCHEMA do Prefect Block...")
            schema_block = Secret.load("deconve-snowflake-schema")
            snowflake_schema = schema_block.get()
            logger.info(f"âœ… Schema carregado do Block 'deconve-snowflake-schema': {snowflake_schema}")
        except Exception as e:
            logger.warning(f"âš ï¸ Erro ao carregar Block 'deconve-snowflake-schema': {e}")
            logger.info("Usando fallback: GOLD")
            snowflake_schema = "GOLD"

    # Valida configuraÃ§Ãµes
    if not api_key:
        raise ValueError("Configure o Block 'deconve-api-key' na UI do Prefect ou DECONVE_API_KEY no .env")

    if not all([snowflake_account, snowflake_user, snowflake_private_key,
                snowflake_warehouse, snowflake_database]):
        raise ValueError("Configure credenciais Snowflake no .env")

    # 2. Calcula datas (se nÃ£o fornecidas manualmente)
    if not start_date or not end_date:
        logger.info(f"ðŸ“… Calculando perÃ­odo: Ãºltimos {days_back} dias...")
        start_date, end_date = calculate_date_range(days_back)

    logger.info(f"ðŸ“… PerÃ­odo: {start_date} a {end_date}")

    # Usa diretÃ³rio temporÃ¡rio que se auto-exclui (mesmo em caso de erro)
    with tempfile.TemporaryDirectory(prefix="deconve_person_flow_") as temp_dir:
        # Monta caminho completo do arquivo CSV temporÃ¡rio
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_filename = f"person_flow_{timestamp}.csv"
        output_path = os.path.join(temp_dir, output_filename)

        logger.info(f"ðŸ“ DiretÃ³rio temporÃ¡rio: {temp_dir}")
        logger.info(f"ðŸ“„ Arquivo temporÃ¡rio: {output_filename}")

        try:
            # 3. Autentica na API
            logger.info("\n" + "=" * 80)
            logger.info("ðŸ” Autenticando na API Deconve...")
            logger.info("=" * 80)

            token_data = authenticate_deconve(api_key)
            access_token = token_data['access_token']

            # 4. ObtÃ©m unidades
            logger.info("\n" + "=" * 80)
            logger.info("ðŸ“¥ Buscando unidades...")
            logger.info("=" * 80)

            units_data = get_units(access_token)
            total_units = units_data.get('total', 0)
            logger.info(f"âœ… {total_units} unidade(s) encontrada(s)")

            # 5. ObtÃ©m lista de cÃ¢meras
            logger.info("\n" + "=" * 80)
            logger.info("ðŸŽ¥ Obtendo lista de cÃ¢meras...")
            logger.info("=" * 80)

            camera_ids = get_all_cameras(access_token, units_data)

            if len(camera_ids) == 0:
                logger.warning("âš ï¸ Nenhuma cÃ¢mera encontrada")
                return {"status": "no_data", "message": "Nenhuma cÃ¢mera encontrada"}

            # 6. Processa cÃ¢meras e salva CSV
            logger.info("\n" + "=" * 80)
            logger.info("ðŸ“Š Processando relatÃ³rios de person flow...")
            logger.info("=" * 80)

            csv_result = process_cameras_and_save_csv(
                access_token=access_token,
                camera_ids=camera_ids,
                start_date=start_date,
                end_date=end_date,
                output_path=output_path,
                group_by=group_by
            )

            rows_written = csv_result.get('rows_written', 0)
            cameras_processed = csv_result.get('cameras_processed', 0)

            # 7. Conecta Snowflake
            logger.info("\n" + "=" * 80)
            logger.info(f"â„ï¸ Conectando Snowflake: {snowflake_account}")
            logger.info("=" * 80)

            snowflake_conn = connect_snowflake(
                account=snowflake_account,
                user=snowflake_user,
                private_key=snowflake_private_key,
                warehouse=snowflake_warehouse,
                database=snowflake_database,
                schema=snowflake_schema,
                role=snowflake_role,
                private_key_passphrase=snowflake_private_key_passphrase,
                timeout=60
            )

            # 8. ObtÃ©m schema da tabela
            table_schema = DECONVE_TABLES_SCHEMAS["person_flow"]
            table_name = table_schema["table_name"]
            primary_keys = table_schema["primary_key"]

            # 9. Cria tabela se nÃ£o existe
            logger.info(f"ðŸ—ï¸ Criando tabela {table_name} se nÃ£o existir...")
            create_table_if_not_exists(
                snowflake_conn,
                table_name,
                table_schema["columns"],
                primary_keys
            )

            # 10. Faz MERGE (UPSERT) no Snowflake
            logger.info("\n" + "=" * 80)
            logger.info("ðŸ”„ Carregando dados no Snowflake (MERGE - UPSERT)...")
            logger.info("=" * 80)

            merge_result = merge_csv_to_snowflake(
                snowflake_conn,
                table_name,
                output_path,
                primary_keys,
                csv_encoding='utf-8',
                columns=['ID_CAMERA', 'DT_FLUXO', 'NR_ENTRADA', 'NR_SAIDA', 'DT_CRIACAO']
            )

            rows_inserted = merge_result.get('rows_inserted', 0)
            rows_updated = merge_result.get('rows_updated', 0)

            # 11. Fecha conexÃ£o Snowflake
            close_snowflake_connection(snowflake_conn)

            # 12. CSV temporÃ¡rio serÃ¡ removido automaticamente ao sair do contexto

            # 13. Resumo
            logger.info("\n" + "=" * 80)
            logger.info("ðŸ“Š RESUMO")
            logger.info("=" * 80)

            logger.info(f"âœ… Registros extraÃ­dos da API: {rows_written}")
            logger.info(f"ðŸ“ Inseridos no Snowflake: {rows_inserted}")
            logger.info(f"ðŸ”„ Atualizados no Snowflake: {rows_updated}")
            logger.info(f"ðŸŽ¥ CÃ¢meras processadas: {cameras_processed}")
            logger.info(f"ðŸ“… PerÃ­odo: {start_date} a {end_date}")
            logger.info(f"â„ï¸ Tabela: {snowflake_database}.{snowflake_schema}.{table_name}")

            duration = time.time() - start_time
            logger.info(f"â±ï¸ DuraÃ§Ã£o: {duration:.1f}s")
            logger.info("=" * 80 + "\n")

            # 14. ARTIFACTS: Visibilidade no Prefect UI
            try:
                # Tabela de resumo
                status_icon = "âœ…" if rows_written > 0 else "âš ï¸"

                table_data = [{
                    "MÃ©trica": "Status",
                    "Valor": f"{status_icon} {'Sucesso' if rows_written > 0 else 'Sem dados'}"
                }, {
                    "MÃ©trica": "Registros ExtraÃ­dos",
                    "Valor": f"{rows_written:,}"
                }, {
                    "MÃ©trica": "Novos Inseridos",
                    "Valor": f"{rows_inserted:,}"
                }, {
                    "MÃ©trica": "Atualizados",
                    "Valor": f"{rows_updated:,}"
                }, {
                    "MÃ©trica": "CÃ¢meras Processadas",
                    "Valor": f"{cameras_processed:,}"
                }, {
                    "MÃ©trica": "PerÃ­odo",
                    "Valor": f"{days_back} dias retroativos"
                }, {
                    "MÃ©trica": "DuraÃ§Ã£o",
                    "Valor": f"{duration:.1f}s"
                }, {
                    "MÃ©trica": "Tabela Snowflake",
                    "Valor": f"{snowflake_database}.{snowflake_schema}.{table_name}"
                }]

                artifact_desc = f"Person Flow: {rows_written:,} registros | {cameras_processed} cÃ¢meras"
                create_table_artifact(
                    key="deconve-person-flow-metrics",
                    table=table_data,
                    description=artifact_desc
                )
            except Exception as e:
                logger.warning(f"Erro criando artifact de tabela: {e}")

            # 15. Envia alerta de sucesso
            if send_alerts:
                try:
                    from prefect.context import get_run_context
                    try:
                        context = get_run_context()
                        job_id = str(context.flow_run.id) if hasattr(context, 'flow_run') else None
                    except Exception:
                        job_id = None

                    alert_sent = send_flow_success_alert(
                        flow_name="ExtraÃ§Ã£o Deconve Person Flow",
                        source="Deconve API",
                        destination=f"Snowflake - {snowflake_database}.{snowflake_schema}.{table_name}",
                        summary={
                            "records_extracted": rows_written,
                            "records_inserted": rows_inserted,
                            "records_updated": rows_updated,
                            "cameras_processed": cameras_processed,
                            "period": f"{start_date} a {end_date}"
                        },
                        duration_seconds=duration,
                        job_id=job_id,
                        group_id=alert_group_id
                    )

                    if alert_sent:
                        logger.info("âœ… Alerta de sucesso enviado")
                    else:
                        logger.warning("âš ï¸ Falha ao enviar alerta (verifique conexÃ£o com API de mensagens)")
                except Exception as alert_error:
                    logger.warning(f"âš ï¸ Erro ao enviar alerta: {alert_error}")

            return {
                "status": "success",
                "records_extracted": rows_written,
                "records_inserted": rows_inserted,
                "records_updated": rows_updated,
                "cameras_processed": cameras_processed,
                "period": f"{start_date} a {end_date}",
                "duration_seconds": duration
            }

        except Exception as e:
            logger.error(f"âŒ Erro: {str(e)}")

            duration = time.time() - start_time

            if send_alerts:
                try:
                    from prefect.context import get_run_context
                    try:
                        context = get_run_context()
                        job_id = str(context.flow_run.id) if hasattr(context, 'flow_run') else None
                    except Exception:
                        job_id = None

                    alert_sent = send_flow_error_alert(
                        flow_name="ExtraÃ§Ã£o Deconve Person Flow",
                        source="Deconve API",
                        destination=f"Snowflake - {snowflake_database or 'N/A'}",
                        error_message=str(e),
                        duration_seconds=duration,
                        job_id=job_id,
                        group_id=alert_group_id
                    )

                    if alert_sent:
                        logger.info("âœ… Alerta de erro enviado")
                    else:
                        logger.warning("âš ï¸ Falha ao enviar alerta de erro (verifique conexÃ£o com API de mensagens)")
                except Exception as alert_error:
                    logger.warning(f"âš ï¸ Erro ao enviar alerta de erro: {alert_error}")

            raise
    # Ao sair do with, o diretÃ³rio temporÃ¡rio Ã© automaticamente excluÃ­do


# Deployment do flow
if __name__ == "__main__":
    # ExecuÃ§Ã£o local para teste
    # deconve_person_flow_to_snowflake()

    # Para fazer deploy:
    deconve_person_flow_to_snowflake.from_source(
        source=".",
        entrypoint="flows/deconve/deconve_person_flow_to_snowflake.py:deconve_person_flow_to_snowflake"
    ).deploy(
        name="deconve-person-flow-to-snowflake",
        work_pool_name="local-pool",
        schedules=[
            CronSchedule(cron="1 0 * * *", timezone="America/Sao_Paulo")
        ],
        tags=["deconve", "api", "snowflake", "gold", "fact"],
        parameters={
            "days_back": 3  # Processa Ãºltimos 3 dias (retroativo)
        },
        description="Pipeline: Deconve API -> Snowflake (DECONVE_PERSON_FLOW) com MERGE",
        version="1.0.0"
    )
